# Sarcasm Detection using BERT, RoBERTa, and LSTM

# üìå Project Overview

This project focuses on sarcasm detection in text using transformer-based architectures such as BERT, RoBERTa, and their LSTM-augmented versions. The models were trained and evaluated on a dataset of 39,386 samples, optimizing accuracy, precision, recall, and F1-score to enhance text classification performance.

# üõ†Ô∏è Technologies Used

NLP Models: BERT, RoBERTa, LSTM

Libraries: PyTorch, TensorFlow, Transformers, Scikit-learn

Dataset Size: 39,386 samples

Performance Metrics: Accuracy, Precision, Recall, F1-score

# üìä Model Performance Comparison
![image](https://github.com/user-attachments/assets/f7814c58-78c2-4564-9be3-6df1892b7628)

Best Accuracy: BERT + LSTM (80.73%)

Best Precision: RoBERTa (80.84%)

Best F1 Score: RoBERTa + LSTM (80.71%)
